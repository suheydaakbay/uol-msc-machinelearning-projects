{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c113f396",
   "metadata": {},
   "source": [
    "# Machine Learning Coursework 2\n",
    "\n",
    "For coursework 2 you will be asked to train and evalute several different classifiers: Naïve Bayes classifier, Random Forest classifier, and kNN classifier using the iris dataset. You will be asked to answer a series of questions relating to each individual model and questions comparing each model. \n",
    "\n",
    "#### You are free to use the sklearn library. \n",
    "\n",
    "\n",
    "Notes:\n",
    "- Remember to comment all of your code (see here for tips: https://stackabuse.com/commenting-python-code/). You can also make use of Jupyter Markdown, where appropriate, to improve the layout of your code and documentation.\n",
    "- Please add docstrings to all of your functions (so that users can get information on inputs/outputs and what each function does by typing SHIFT+TAB over the function name. For more detail on python docstrings, see here: https://numpydoc.readthedocs.io/en/latest/format.html)\n",
    "- When a question allows a free-form answer (e.g. what do you observe?), create a new markdown cell below and answer the question in the notebook. \n",
    "- Always save your notebook when you are done (this is not automatic)!\n",
    "- Upload your completed notebook using the VLE\n",
    "\n",
    "Plagiarism: please make sure that the material you submit has been created by you. Any sources you use for code should be properly referenced. Your code will be checked for plagiarism using appropriate software.\n",
    "\n",
    "### Marking \n",
    "\n",
    "The grades in this coursework are allocated approximately as follows:\n",
    "\n",
    "|                                                    | mark  |  \n",
    "|----------------------------------------------------|-------|\n",
    "| Code                                               | 7     |\n",
    "| Code Report/comments                               | 6     |\n",
    "| Model questions                                    | 14    |  \n",
    "| Model comparision questions                        | 18    |\n",
    "| Total available                                    |**45** |  \n",
    "\n",
    "##### Remember to save your notebook as “CW2.ipynb”. It is a good idea to re-run the whole thing before saving and submitting. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f1b29",
   "metadata": {},
   "source": [
    "## 1. Classifiers [7 marks total]\n",
    "Code and train your three classifiers in the cells below the corresponding header. You do not need to implement cross-validation in this coursework, simply fit the data. You are free to use sklearn and other packages where necessary.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "794c154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# import datasets\n",
    "from sklearn import datasets \n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris() # load data \n",
    "print(iris.DESCR) # print dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442af0b",
   "metadata": {},
   "source": [
    "The iris dataset consists of 4 different features and these columns contain continuous numerical values. Prediction values are classified into 3 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896bb6a",
   "metadata": {},
   "source": [
    "The features of the data set are taken with the variable X, and the target column with the variable y. For the learning and evaluation stages of the model, the test data is separated as *30 percent* of the entire data set with the `train_test_split` function in the `model_selection` module of `sklearn`. The `random_state` value is set to deal with the same train and test data on each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b63e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d097b4",
   "metadata": {},
   "source": [
    "### 1.1 Naïve Bayes Classifier [2]\n",
    "Train a naïve bayes classifier in python. \n",
    "\n",
    "Use your code to fit the data given above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b2dad",
   "metadata": {},
   "source": [
    "To generate a **Gaussian Naive Bayes** model, the `GaussianNB` class is imported and stored with the `NB_model` variable. Then the model is fitted with the training set of the feature and label data. \n",
    "With the `predict` method of the model, y values are estimated using the *x_test* set and the predicted values are stored in the `y_pred_NB` variable.\n",
    "\n",
    "In order to evaluate the performance of the label values predicted by the model, the *accuracy*, *f1 score*, *precision*, *recall* values of the `metrics` module of `sklearn` lilbrary are calculated and a *confusion matrix* is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3902ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555555555555556\n",
      "Presicion: 0.9555555555555556\n",
      "Recall: 0.9555555555555556\n",
      "F1 Score: 0.9555555555555556\n",
      "Confusion matrix \n",
      " [[16  0  0]\n",
      " [ 0 16  1]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_pred_NB = NB_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_NB))\n",
    "print(\"Presicion:\", precision_score(y_test, y_pred_NB, average = 'micro')) #micro average computes a global average\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_NB, average = 'micro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_NB, average = 'micro'))\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12049a11",
   "metadata": {},
   "source": [
    "### 1.2 Random Forst Classifier [3]\n",
    "Train a random forest classifier in python. Use your code to fit the data given above. \n",
    "\n",
    "Evaluate feature performance of the model. \n",
    "\n",
    "Visualise the feature importance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5620a",
   "metadata": {},
   "source": [
    "**Random Forest** model, which is an ensemble method, is initialized by importing the `RandomForestClassifier` class. The model is created with the variable named `RF_model` by setting the `n_estimators` parameter to 200. The model is trained with the `fit` method using *X_train* and *y_train*, and the label values to be estimated are created with the `predict` method. Actual and predicted values are evaluated with `metrics` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "855200cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555555555555556\n",
      "Presicion: 0.9555555555555556\n",
      "Recall: 0.9555555555555556\n",
      "F1 Score: 0.9555555555555556\n",
      "Confusion matrix \n",
      " [[16  0  0]\n",
      " [ 0 16  1]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators = 200, random_state= 1234) #number of trees are 200\n",
    "RF_model.fit(X_train, y_train)\n",
    "y_pred_RF = RF_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_RF)),\n",
    "print(\"Presicion:\", precision_score(y_test, y_pred_RF, average = 'micro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_RF, average = 'micro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_RF, average = 'micro'))\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb06fb",
   "metadata": {},
   "source": [
    "To find the most important feature, first the `feature_importances_` property under the `RandomForestClassifier` class is called. This property prints the importance scores of the column values. The importance scores of the features are sorted in descending order and the values are converted to `Dataframe` with the index values are the names of independent columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ea0729f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>petal length (cm)</th>\n",
       "      <td>0.462464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal width (cm)</th>\n",
       "      <td>0.403571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <td>0.099134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <td>0.034830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature importances\n",
       "petal length (cm)             0.462464\n",
       "petal width (cm)              0.403571\n",
       "sepal length (cm)             0.099134\n",
       "sepal width (cm)              0.034830"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "importances = RF_model.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "forest_importances.sort_values(ascending=False, inplace = True)\n",
    "data = pd.DataFrame({\n",
    "    \"feature importances\": forest_importances\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab4191",
   "metadata": {},
   "source": [
    "Feature importance scores are visualized by the *barchart* method with the `pyplot` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6deaf23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAD4CAYAAAB10khoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWtUlEQVR4nO3de7ClVZ3e8e8zgFwE8QKJDY4eB4GRi9xaEkQYNE5iwRRqiTGRQQjWGAcvMYY4lOOtvIImXsobA4YwKjNeKImMXYJ44aKo0K3ddLfYKNoZRCpolJaEiwP88sdePdl9ON1n73NOn92s/n6qus67117vWr+9qumH9b7vOSdVhSRJPfu9SRcgSdLWZthJkrpn2EmSumfYSZK6Z9hJkrq346QL0Mz22muvmpqamnQZkvSIsmLFil9V1d7T2w27bdTU1BTLly+fdBmS9IiS5H/O1O5lTElS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3/KbybdTq2zcwdc6ySZchqSPrzz1p0iVMjDs7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcWLeySnJFknxH6XZzklDmM/6okL5+hfSrJmnZ8eJITh957e5KzRxg7Sb6R5DHj1jXDWF9L8rj5jiNJGt1i7uzOAGYNu7mqqvOr6lOzdDscOHGWPjM5EVhVVb+dw7nTfRo4awHGkSSNaE5h13ZLP0pySZKbk1yaZLf23lFJrkmyIsmVSZa0ndpS4JIkK5PsmuStSW5MsibJBUmyhfn+SZIV7fiwJJXkye31rUl2G96ltRpWJVkFvLq1PQp4B/DSVsNL2/AHJbk6yU+TvG4zJZwKfGmonpcnuanN8enWdnGSTyT5bhvrhCQXtfW5eGisy4F/O+aSS5LmYT47uwOBj1fV04HfAmcl2Qn4CHBKVR0FXAS8u6ouBZYDp1bV4VV1L/DRqnpmVR0C7Ar8yeYmqqo7gV3aZcTj2ljHJXkKcGdV3TPtlP8OvLaqDhsa43fAW4HPtRo+1976Q+BfAUcDb2ufYbpjgY1hezDwZuC5bfz/MNTvccAxwH9kEGofBA4GDk1yeKvjN8DOSZ4wfZIkr0yyPMnyB+/ZsLnlkCSNaT5hd1tVfbsdfwZ4NoMAPAS4KslKBqHwpM2c/5wk30uyGngug1DYkusZhM7xwHva1+OA64Y7JXks8NiqurY1fXqWcZdV1f1V9SvgTuCfztDn8VV1dzt+LvCF1p+q+vVQv7+rqgJWA/+rqlZX1UPAWmBqqN+dzHBJt6ouqKqlVbV0h932nKVsSdKodpzHuTXD6wBrq+qYLZ2YZBfg48DSqrotyduBXWaZ71oG4fYUBpcU/6LNuWz80jdx/9Dxg8y8Jg8k+b0WXKOM9dC0cR+aNu4uwL3jFipJmpv57OyenGRjqL0M+BawDth7Y3uSndplP4C7gT3a8cZg+1WS3YFRnr68DvhT4MctdH7N4MGRbw13qqq7gLuSPLs1nTr09nAN41gH/EE7/gbwko2XIZM8fpyB2r3JJwLr51CHJGkO5hN264BXJ7mZwb2qT7T7YqcA57WHQ1YCz2r9LwbOb5c37wcuBNYAVwI3zjZZVa1nsHPceHnyW8Bd7R7YdP8O+Fiba/jBl28yeCBl+AGVUSwDTmh1rAXeDVzTPuMHxhgH4Cjgu1X1wJjnSZLmKINbTGOelEwBX24Pl3QvyRLgU1X1xwsw1oeBy6vq61vqt/OS/WvJ6R+a73SS9I/Wn3vSpEvY6pKsqKql09v9CSojqKo7gAsX4pvKgTWzBZ0kaWHN6QGVdklxu9jVbVRVn1+gcS5ciHEkSaNzZydJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6t6cfuuBtr5D992T5dvB756SpMXgzk6S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9f4LKNmr17RuYOmfZpMuQpEW1fiv95Ch3dpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTubbWwS3JGkn1G6HdxklNGbV+Aut40dDyVZM2I570+ycsXYP7XJDlzvuNIkka3NXd2ZwCzht0EvGn2LptKsiNwJvA3CzD/RcBrF2AcSdKIRgq7tgP6UZJLktyc5NIku7X3jkpyTZIVSa5MsqTtyJYClyRZmWTXJG9NcmOSNUkuSJJRi5xpjtZ+dZLzktyQ5JYkx7X23ZJ8PskPk1yW5HtJliY5F9i11XRJG36HJBcmWZvkq0l2naGE5wLfr6oH2vhPS/K1JKuSfD/JfklOaDV+KclPk5yb5NRW2+ok+wFU1T3A+iRHj/r5JUnzM87O7kDg41X1dOC3wFlJdgI+ApxSVUcx2LW8u6ouBZYDp1bV4VV1L/DRqnpmVR0C7Ar8ySiTbm6OoS47VtXRwOuBt7W2s4DfVNVBwFuAowCq6hzg3lbTqa3v/sDHqupg4C7gxTOUcSywYuj1Je2cw4BnAXe09sOAVwFPB04DDmi1fZJNd3PLgeNm+KyvTLI8yfIH79mwxXWRJI1uxzH63lZV327HnwFeB1wBHAJc1TZqO/D//+Gf7jlJ3gjsBjweWAv83QjzHjjLHF9sX1cAU+342cCHAapqTZKbtjD+z6pq5QxjDFsC3AyQZA9g36q6rI1/X2sHuLGq7mivbwW+2s5fDTxnaLw7gT+cPklVXQBcALDzkv1rCzVLksYwTthN/8e3gABrq+qYLZ2YZBfg48DSqrotyduBXUacd7Y57m9fH2S8zzP9/I1jzHQZ815Gq3d4rIeGXj80rbZd2piSpEUwzmXMJyfZGDgvA74FrAP23tieZKckB7c+dwN7tOONQfGrJLsD4zxluaU5NufbwL9u/Q8CDh167x/apdFx3Aw8DaCq7gZ+nuSFbfydN96/HMMBwEhPgUqS5m+csFsHvDrJzcDjgE9U1e8YBNd5SVYBKxncwwK4GDg/yUoGO5wLGfwDfyVw46iTzjLH5nycQUD+EHgXg0umG2+CXQDcNPSAyii+Ahw/9Po04HXt8uj1wBPHGAsG9wCvGvMcSdIcpWr2W0NJpoAvt4dLtnlJdgB2qqr72lOQXwMObME51zEvA95YVT+eZ21HAG+oqtO21G/nJfvXktM/NJ+pJOkRZ/25J83r/CQrqmrp9Pa53ON6JNgN+Ga7XBngrPkEXXMOgwdV5hV2wF4MnhCVJC2SkcKuqtYzeCLyEaHdV3tYss9zzHUMLuXOdxwvX0rSIvNnY0qSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSutfrr/h5xDt03z1ZPs/f6yRJGnBnJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p4/QWUbtfr2DUyds2yrjb/en84iaTvizk6S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUvW0u7JKckOTLczhvnySXbua9q5MsbcdvGmqfSrJmxPFfn+Tl49Y1wzivSXLmfMeRJI1umwu7uaqqX1TVKSN0fdPsXTaVZEfgTOBvxi7s4S4CXrsA40iSRjR22CV5dJJlSVYlWZPkpa39qCTXJFmR5MokS1r71Uk+nGRl6390az86yXeS/CDJ9UkOnGXeZUme0Y5/kOSt7fgdSf5seJeWZNckn01yc5LLgF1b+7nArq2WS9rQOyS5MMnaJF9NsusM0z8X+H5VPdDGeVqSr7U1+H6S/dqO9JokX0ry0yTnJjk1yQ1JVifZD6Cq7gHWb1wHSdLWN5ed3fOBX1TVYVV1CHBFkp2AjwCnVNVRDHYv7x46Z7eqOhw4q70H8CPguKo6Angr8J5Z5r0OOC7JnsADwLGt/Tjg2ml9/xy4p6qeDrwNOAqgqs4B7q2qw6vq1NZ3f+BjVXUwcBfw4hnmPhZYMfT6knbOYcCzgDta+2HAq4CnA6cBB1TV0cAn2XQ3t7zVvYkkr0yyPMnyB+/ZsKW1kCSNYS5htxr44yTnJTmuqjYABwKHAFclWQm8GXjS0Dl/C1BV1wKPSfJYYE/gC2039kHg4FnmvQ44nkHwLAN2T7Ib8NSqWjet7/HAZ9qcNwE3bWHcn1XVyna8Apiaoc8S4JcASfYA9q2qy9r497XdGsCNVXVHVd0P3Ap8tbWvnjbuncA+0yepqguqamlVLd1htz23ULIkaRw7jntCVd2S5EjgROBdSb4OXAasrapjNnfaDK/fCXyzql6UZAq4epapbwSWAj8FrgL2Av6MTXdcc3H/0PGDtEue09wL7DLmWA8NvX6ITdd6lzamJGkRzOWe3T4MLhF+Bng/cCSwDtg7yTGtz05JhndqG+/rPRvY0HaDewK3t/fPmG3eqvodcBvwEuA7DHZ6Z/PwS5i0tpe1OQ8BnjH03j+0y67juBl4WqvjbuDnSV7Yxt+57TDHcQAw0lOgkqT5m8tlzEOBG9rlyrcB72pBdApwXpJVwEoG97I2ui/JD4DzgVe0tvcB723to+4wrwPurKp72/GT2tfpPsHgMufNwDvYdPd3AXDT0AMqo/gKg0ujG50GvC7JTcD1wBPHGAsGl2KvGvMcSdIcpWr6FcYFniC5Gji7qpZv1Ym2svZU5xur6sfzHOcI4A1VddqW+u28ZP9acvqH5jPVFq0/96StNrYkTUqSFVW1dHp7N99ntwjOYfCgynztBbxlAcaRJI1o7AdUxlVVJ2ztORZDe+Jz+lOfcxnHy5eStMjc2UmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSurfVf+uB5ubQffdkub9zTpIWhDs7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9/wJKtuo1bdvYOqcZZMuY5ux3p8mI2ke3NlJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSujfRsEtyQpIvj9q+APO9MMlBQ6+vTrJ0hPOWLEQ9SfZOcsV8x5EkjWd729m9EDhotk4zeANw4Xwnr6pfAnckOXa+Y0mSRrfFsEvy6CTLkqxKsibJS1v7UUmuSbIiyZVJlrT2q5N8OMnK1v/o1n50ku8k+UGS65McOGqBrYaLktzQzn9Baz8jyReTXJHkx0neN3TOK5Lc0s65MMlHkzwLOBl4f6tvv9b9Ja3fLUmO20wZLwauaGPvkOS/tM93U5LXtvb1Sd7bxl6e5Mi2NrcmedXQWP8DOHXUzy9Jmr8dZ3n/+cAvquokgCR7JtkJ+Ajwgqr6ZQvAdwNntnN2q6rDkxwPXAQcAvwIOK6qHkjyPOA9DAJkFH8JfKOqzkzyWOCGJF9r7x0OHAHcD6xL8hHgQeAtwJHA3cA3gFVVdX2Sy4EvV9Wl7fMA7FhVRyc5EXgb8LzhyZM8FfhNVd3fml4JTAGHt8/z+KHuf98++weBi4FjgV2ANcD5rc9y4F0zfdAkr2zjs8Nj9h5xeSRJs5kt7FYD/zXJeQxC4rokhzAIsKtaWOwA3DF0zt8CVNW1SR7TAmoP4K+T7A8UsNMYNf5L4OQkZ7fXuwBPbsdfr6oNAEl+CDwF2Au4pqp+3dq/ABywhfG/2L6uYBBi0y0Bfjn0+nnA+VX1QPucvx567/L2dTWwe1XdDdyd5P4kj62qu4A7gX1mKqSqLgAuANh5yf61hZolSWPYYthV1S1JjgROBN6V5OvAZcDaqjpmc6fN8PqdwDer6kVJpoCrx6gxwIurat0mjck/Y7Cj2+hBZg/vmWwcY3Pn38sgYMcZ66FptT00NPYubUxJ0iKZ7Z7dPsA9VfUZ4P0MLg2uA/ZOckzrs1OSg4dO23hf79nAhrbz2hO4vb1/xpg1Xgm8Nm0bmeSIWfrfCPxRkscl2ZFNL5fezWCXOY5b2HTHdxXw79vYTLuMOYoDGFzWlCQtktmexjyUwT2ylQzuZ72rqn4HnAKcl2QVsBJ41tA59yX5AYN7VK9obe8D3tvax919vZPBZc+bkqxtrzerqm5ncE/wBuDbwHpgQ3v7s8B/bg+67DfzCA8b7/8CtyZ5Wmv6JPD3rZ5VwMvG+zg8B1g25jmSpHlI1cLdGkpyNXB2VS1fsEHnVsfuVfV/2u7rMuCiqrpsHuO9CDiqqt68ALVdy+Dhnt9sqd/OS/avJad/aL7TdWP9uSdNugRJjwBJVlTVw75/utfvs3t7242uAX7G4HH/OWtBuX6+RSXZG/jAbEEnSVpYc3mgY7Oq6oSFHG+uqurs2XuNPeYnF2CMXzLP4JUkja/XnZ0kSf/IsJMkdc+wkyR1z7CTJHXPsJMkdc+wkyR1z7CTJHXPsJMkdc+wkyR1z7CTJHXPsJMkdc+wkyR1z7CTJHVvQX/rgRbOofvuyXJ/h5skLQh3dpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO6lqiZdg2aQ5G5g3aTr2MbsBfxq0kVsY1yTh3NNHm57WpOnVNXe0xv9cWHbrnVVtXTSRWxLkix3TTblmjyca/JwromXMSVJ2wHDTpLUPcNu23XBpAvYBrkmD+eaPJxr8nDb/Zr4gIokqXvu7CRJ3TPsJEndM+wmLMnzk6xL8pMk58zw/s5JPtfe/16SqQmUuahGWJPjk3w/yQNJTplEjYtthDV5Q5IfJrkpydeTPGUSdS6mEdbkVUlWJ1mZ5FtJDppEnYtltvUY6vfiJJVk+/pWhKryz4T+ADsAtwJ/ADwKWAUcNK3PWcD57fjfAJ+bdN3bwJpMAc8APgWcMumat5E1eQ6wWzv+c/+eFMBjho5PBq6YdN2TXI/Wbw/gWuC7wNJJ172Yf9zZTdbRwE+q6qdV9Tvgs8ALpvV5AfDX7fhS4F8kySLWuNhmXZOqWl9VNwEPTaLACRhlTb5ZVfe0l98FnrTINS62Udbkt0MvHw30/DTeKP+WALwTOA+4bzGL2xYYdpO1L3Db0Ouft7YZ+1TVA8AG4AmLUt1kjLIm25tx1+QVwFe2akWTN9KaJHl1kluB9wGvW6TaJmHW9UhyJPD7VbVsMQvbVhh2UkeS/CmwFHj/pGvZFlTVx6pqP+AvgDdPup5JSfJ7wAeA/zTpWibFsJus24HfH3r9pNY2Y58kOwJ7Av97UaqbjFHWZHsz0pokeR7wl8DJVXX/ItU2KeP+Pfks8MKtWdCEzbYeewCHAFcnWQ/8c+Dy7ekhFcNusm4E9k/y1CSPYvAAyuXT+lwOnN6OTwG+Ue1Oc6dGWZPtzaxrkuQI4K8YBN2dE6hxsY2yJvsPvTwJ+PEi1rfYtrgeVbWhqvaqqqmqmmJwX/fkqlo+mXIXn2E3Qe0e3GuAK4Gbgc9X1dok70hycuv234AnJPkJ8AZgs48U92CUNUnyzCQ/B14C/FWStZOreOsb8e/J+4HdgS+0R+27/h+EEdfkNUnWJlnJ4L+d02ce7ZFvxPXYrvnjwiRJ3XNnJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nq3v8DgoUGKQjCSgAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.barh(iris.feature_names, RF_model.feature_importances_)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579fd089",
   "metadata": {},
   "source": [
    "The least significant column is removed from the dataset. The data is again divided into training and test sets. With this state of the data, **Random Forest Classifier** model is created and model performance is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae5efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n",
      "Presicion: 0.9777777777777777\n",
      "Recall: 0.9777777777777777\n",
      "F1 Score: 0.9777777777777777\n",
      "Confusion matrix \n",
      " [[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier without less important feature\n",
    "import pandas as pd\n",
    "\n",
    "data = iris\n",
    "df_dropped = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "df_dropped.drop(['sepal width (cm)'], axis=1, inplace=True)\n",
    "X_dropped = df_dropped\n",
    "y = iris.target\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_dropped, y, test_size=0.3, random_state=0)\n",
    "\n",
    "RF_model2 = RandomForestClassifier()\n",
    "RF_model2.fit(X_train_d, y_train_d)\n",
    "y_pred_RF2 = RF_model2.predict(X_test_d)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_d, y_pred_RF2))\n",
    "print(\"Presicion:\", precision_score(y_test_d, y_pred_RF2, average = 'micro'))\n",
    "print(\"Recall:\", recall_score(y_test_d, y_pred_RF2, average = 'micro'))\n",
    "print(\"F1 Score:\", f1_score(y_test_d, y_pred_RF2, average = 'micro'))\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test_d, y_pred_RF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4cb46e",
   "metadata": {},
   "source": [
    "### 1.3 kNN Classifier [2]\n",
    "Train a kNN classifier in python. \n",
    "\n",
    "Use your code to fit the data given above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d17ba",
   "metadata": {},
   "source": [
    "Finally, the `KNeighborsClassifier` class is called with the `neighbors` module of `sklear` and the `n_neighbors` parameter is taken as 3 and stored with the `KNN_model` variable. With the training sets, the model is fit and the y values predicted by the model are stored with `y_pred_KNN`. Model performance is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba99c00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n",
      "Presicion: 0.9777777777777777\n",
      "Recall: 0.9777777777777777\n",
      "F1 Score: 0.9777777777777777\n",
      "Confusion matrix \n",
      " [[16  0  0]\n",
      " [ 0 17  0]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=3) #the best accuracy value is obtained when the number of neighbors is 3.\n",
    "KNN_model.fit(X_train, y_train)\n",
    "y_pred_KNN = KNN_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_KNN))\n",
    "print(\"Presicion:\", precision_score(y_test, y_pred_KNN, average = 'micro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_KNN, average = 'micro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_KNN, average = 'micro'))\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test, y_pred_KNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877298a3",
   "metadata": {},
   "source": [
    "## 2 Code Report [6 marks total]\n",
    "In a markdown box, write a short report (no more than 500 words) that describes the workings of your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86180ce",
   "metadata": {},
   "source": [
    "The `Gaussian Naive Bayes Classifier` algorithm was used as a solver. Gaussian type is chosen because the predictors have continuous values. In addition, since the predictors are independent of each other, this algorithm is appropriate to use in the iris data set. The Naive Bayes algorithm assigns the samples to be predicted to a class by calculating the probability of each attribute belonging to each class in the training set and works as follows: first, it separates the rows in the data set according to their class values and calculates the *mean* and *standard deviation* values for each column on these separated states. It then calculates the *Gaussian probability distribution* function for each row. Calculates the prediction probabilities of each class for the data to be predicted. The class value with the most probability will be the value assigned.\n",
    "\n",
    "Basically, the `Random Forest Classification` algorithm, which we can say consists of many *decision trees*, first takes the data in the dataset in a mixed order and repeats more than once (there may be rows that are not included) which is *boostrap* method and creates more than one sub-dataset. Optimally, one of the selected columns is taken as root and the tree is created as usual, but only considering a random subset of variables at each step. It is created multiple times from the boostraped dataset and decision trees are created with these datasets multiple times. Then, the data set to be predicted is used in these decision trees, and the most repeated decision among the results is the class value determined for the predicted rows.\n",
    "\n",
    "Finally, the `K-Nearest Neighbors` algorithm used works like this, initially the number of *k neighbors* is determined randomly. The distance of the k nearest points to the desired point is calculated. Assuming that the formula used to calculate the distance is *euclidean*; Take the difference of the values of two points in the same column and square this value. It does this for each feature and the squared values are summed, then the square root of the final value is taken. Calculate how many points *(frequency)* of which class are in k points. The most common class value is assigned as the class value of the point to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03db53",
   "metadata": {},
   "source": [
    "## 3 Model Questions [14 marks total]\n",
    "Please answer the following questions relating to your classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8eecce",
   "metadata": {},
   "source": [
    "### 3.1 Naïves Bayes Questions [4]\n",
    "Why do zero probabilities in our Naïve Bayes model cause problems? \n",
    "\n",
    "How can we avoid the problem of zero probabilities in our Naïve Bayes model? \n",
    "\n",
    "Please answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59490d2",
   "metadata": {},
   "source": [
    "When estimating on the model with test data, if the training has a value that it does not see in the data set, the probability of the occurrence of that class is considered 0. In probability multiplication using the missing labeled point, our result is 0. If our data set consisted of *categorical* values, a solution called `Laplace Smooting` could be used to add a number to the data. When we add a number to the occurrence of the categorical value in the data set, the *likelihood probability* is similar to a *uniform distribution* and the relative frequencies of the classes are not affected. But the iris dataset consists of *continuous* and *unlimited* data. If we try to use the Laplace Smooting method in such a data set, we get a *Gaussian distribution* with infinite variance. In other words, Laplace Smooting is not a solution when Gaussian Naive Bayes algorithm is used instead of `Multinomial Naive Bayes`. Apart from this, observations with a mean much lower than the mean of the curve may not be given weight in the *Gaussian calculation*. The `var_smoothing` variable, which is a Gaussian Naive Bayes parameter, allows us to change the variance of the distribution. We can give this parameter a dummy value to smooth the *Gaussian curve* and change the likelyhood of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89f91b",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest Questions [6]\n",
    "Which feature is the most important from your random forest classifier? \n",
    "\n",
    "Can any features be removed to increase accuracy of the model, if so which features? \n",
    "\n",
    "Explain why it would be useful to remove these features. \n",
    "\n",
    "Please answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72a631",
   "metadata": {},
   "source": [
    "Using all the features of the iris data set with the **Random Forest** algorithm, 2 of the 45 predicted label values ​​were assigned to the wrong class and we got 0.95 accuracy. We then ranked the least important columns by feature sports using the `feature_importances_` property and determined that the least important independent column was `'sepal width'` (0.03 score). With the **Random Forest** algorithm, which was applied again after this column was dropped, only 1 class assignment was incorrect, so we obtained a higher accuracy value (0.97).\n",
    "This result shows us that with feature selection, redundant and irrelevant data is removed, which increases learning accuracy. As the overfitting risk increases as the number of features increases, the overfitting problem is avoided. Since the training time increases exponentially with the number of features, the model training time becomes faster when target values ​​are created with only the most useful inputs. It also makes it easier to understand the data set and the results obtained, and learning costs are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb16099",
   "metadata": {},
   "source": [
    "### 3.3 kNN Questions [4]\n",
    "Do you think the kNN classifier is best suited to the iris dataset? \n",
    "\n",
    "What ideal qualities would the most appropriate dataset display?  \n",
    "\n",
    "Please answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b18601",
   "metadata": {},
   "source": [
    "`K-Nearest Neighbors` algorithm can be used in recognition of patterns and consistency in data for both classification and regression problems. The KNN classification method is a *supervised learning* algorithm that can achieve very good results with small datasets (fewer than 100K labeled non-textual data), is very sensitive to outliers, and cannot cope well with lost data. The iris we have is quite standard (as can be seen from the boxplot), a dataset with very few features that does not contain missing data and can be solved with the classification model. In this respect, the iris dataset is quite suitable for the KNN algorithm. The highest accuracy value was obtained by using the KNN algorithm (0.97). (The state before the feature selection method is applied in the Random Forest algorithm is taken into account)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90533f",
   "metadata": {},
   "source": [
    "The missing values in the dataset are checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18147cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing data in iris dataset\n",
    "X_ = pd.DataFrame(X)\n",
    "X_.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80425b6e",
   "metadata": {},
   "source": [
    "The value ranges of the cells in the data set are checked to see some statistical implications of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a303df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check diversity in iris dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(X)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518fe4c",
   "metadata": {},
   "source": [
    "## 4 Comparing Models [18 marks total]\n",
    "Please answer the following questions comparing your classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73fc05",
   "metadata": {},
   "source": [
    "### 4.1 Compare each model [3]\n",
    "What differences do you see between your Naïve Bayes classifier, your random forest classifier, and your kNN classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8b3da",
   "metadata": {},
   "source": [
    "These 3 models created using the iris dataset are classification algorithms used in *supervised learning*.\n",
    "When you set the `random_state` parameter to 1234, which ensures that the training and test sets are mixed the same every time, the **Gaussian Naive Bayes** model and the *accuracy*, *presicion*, *recall*, *F1 score* metrics have a value of 0.95. It just assigned 2 label values ​​to the wrong class.\n",
    "The same results are obtained when using the **Random Forest** model. However, after removing the least important feature and running the model again, it is seen that the performance of the Random Forest model is 0.97 for all metrics.\n",
    "When **K-Nearest Neighbors** is selected as the model and 3 nearest neighbors are taken for evaluation, only 1 class misclassifies the target value and all performance measures reach 0.97.\n",
    "If we evaluate these three classification algorithms in terms of time, although we get the best performance with KNN, the slowest classification algorithm would be also KNN. Because KNN is memory intensive and therefore it is costly.\n",
    "Also, if we had larger data with various types of input values ​​(discreate numeric, categoric etc.), unnormalized or included missing data, we could not say with certainty that the best algorithm would be KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947cf53",
   "metadata": {},
   "source": [
    "### 4.2 Accuracy [6]\n",
    "Can you explain why there are differences in accuracy between the three classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92bfd08",
   "metadata": {},
   "source": [
    "The accuracy value obtained by dividing the correctly predicted samples by the total number of samples has a value of 0.95 with the GNB and RF algorithms, while it has a value of 0.97 with the KNN algorithm. All three algorithms work with labeled training data, which means they are supervised machine learning algorithms.\n",
    "\n",
    "There are several reasons for obtaining different performance results with different algorithms despite using the same data set. Some algorithms have higher *variance* and want to work with a larger data set. This causes them to be more *sensitive*. This amount of sensitive also affects the number of outputs that the algorithm will classify correctly. It can change the variance of the model by changing the hyperparameters of the model, thus increasing the accuracy value. For example, a higher accuracy value could be obtained by changing the k value in the KNN algorithm. The *randomness* in the dataset also adds to the performance of the algorithms. When dividing the iris dataset into training and test sets, the `random_state` parameter was set to 1234. With this value, we obtain the highest accuracy with the KNN algorithm. If the data set was mixed and divided with different randomness or if the *cross validation* method was also used, the highest performance algorithm could be GNB or RF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f44bc",
   "metadata": {},
   "source": [
    "### 4.3 Appropriate Use [9]\n",
    "When would it be appropriate to use each different classifier? \n",
    "\n",
    "Reference real-world situations and examples of specific data sets and explain why that classifier would be most appropriate for that use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bc740",
   "metadata": {},
   "source": [
    "**Naive Bayes** works better with categorical features, while **Gaussian Naive Bayes** works with continuous valued features. GNB wants features to be completely conditionally independent from each other. It can easily deal with lost data. It also makes a classification as a predictive value.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/jasleensondhi/dog-intelligence-comparison-based-on-size\n",
    "\n",
    "With this data set, it is intended to classify whether dogs are intelligent or not. The features of the dataset are the dog's type, weight, height, upper and lower limits of the new repetition values. Label values ​​'clever' and 'not smart' classes are used. This data set is very suitable for Gaussian Naive Bayes because the features are independent, so for example, increasing the value of one feature does not cause the increase of the other. Also, the data is numeric and continuous type instead of categorical.\n",
    "\n",
    "\n",
    "**Random Forest** can be used for both classification and regression problems. It works better on large datasets. (This also causes computational complexity, causing the model to resolve very slowly) It is far from the overfitting problem that the Decision Tree method suffers from. It can easily deal with missing data using median or weighted average methods.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/reihanenamdari/breast-cancer\n",
    "\n",
    "This dataset is suitable for RFC because it consists of 16 columns and 4024 rows. Considering that we will have fewer observations than the original data when creating this sub-sampleset, it is a large enough data. When variables such as age, T and N stages, tumor size, estrogen and progesterone status are provided, classification can be made with the RF method to predict whether the cancer patient is alive or not.\n",
    "\n",
    "**K-Nearest Neighbors** provide optimum results with small, noise-free and missing values ​​datasets. It makes sense to use a non-parametric algorithm, KNN, when we do not have prior knowledge of the data distribution. However, datasets with skewed distributions, which are common class variables, and multidimensional datasets suffer from the curse of dimensionality, reducing KNN accuracy.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/mjamilmoughal/fruits-with-colors-dataset\n",
    "\n",
    "This fruits with color dataset consists of 60 rows, is quite small, does not contain noise and has a small number of missing cells. It is tried to guess which fruit the input is by looking at the mass, color, width and height values. In this respect, it is quite suitable for classification with the KNN model. In addition, since the number of dimensions of the data set is low, the model will be protected from the curse of dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
